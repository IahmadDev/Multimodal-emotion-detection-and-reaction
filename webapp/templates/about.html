{% extends 'layout.html' %}

{% block body %}
    <div class="p-5 mb-4 bg-light rounded-3">
        <div class="container-fluid py-5 text-center">
            <h1 class="display-5 fw-bold">about</h1>
            <p class="fs-5 text-break">This project was developed by Ronja Rehm and Jan Kühlborn for the FAU IIS Hot
                Topics Seminar.</p>
            <p class="fs-5 text-break">The goal was to find a way to analyze an audio and video stream in Python, find
                suitable features for the emotion recognition and train neural networks. Research has shown that
                convolutional neural networks (CNNs) achieve the best accuracies and performances for both streams.
                Still, due to the very different input forms, we decided to train two separate models whose predictions
                are then combined.</p>
            <p class="fs-5 text-break">For more information, refer to the presentation files in the GitHub
                repository.</p>
        </div>
    </div>
    <div class="p-5 mb-4 bg-light rounded-5">
        <div class="container-fluid py-7">
            <h1 class="display-6 fw-bold">licenses and citations</h1>
            <p class="fs-6 text-break">The project is licensed under the MIT License, Copyright (c) 2022 Chair of
                Technical Information Systems, Friedrich-Alexander-University. The data that we used to train our models
                is free to download on Kaggle and the
                websites of their respective research institutes.</p>
            <p class="fs-6 text-break">Audio Data:</p>
            <ul>
                <li>The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS),
                    released under a Creative Commons Attribution license: <a
                            href="https://zenodo.org/record/1188976">source</a></li>
                <li>Toronto emotional speech set (TESS),
                    this collection is published under Creative Commons license Attribution-NonCommercial-NoDerivatives
                    4.0 International: <a
                            href="https://tspace.library.utoronto.ca/handle/1807/24487">source</a></li>
                <li>Surrey Audio-Visual Expressed Emotion (SAVEE) Database,
                    available free of charge for research purposes: <a
                            href="http://kahlan.eps.surrey.ac.uk/savee/Download.html">source</a></li>
                <li>(Berlin Database of Emotional Speech,
                    no license: <a
                            href="http://emodb.bilderbar.info/docu/#download">source</a>) - not used
                </li>
                <li>(The Crowd-sourced Emotional Mutimodal Actors Dataset (CREMA-D) is made available under the <a
                        href="http://opendatacommons.org/licenses/odbl/1.0/"> Open Database License.</a> Any rights in
                    individual contents of the database are licensed under the Database Contents License:
                    http://opendatacommons.org/licenses/dbcl/1.0/
                    <a href="https://github.com/CheyneyComputerScience/CREMA-D"> source</a>) - not used
                </li>
            </ul>
            <p class="fs-6 text-break">Video Data:</p>
            <ul>
                <li>CK+48 5 emotions, released under MIT License Copyright (c) 2018 WuJie: <a
                        href="https://github.com/WuJie1010/Facial-Expression-Recognition.Pytorch/tree/master/CK%2B48">source</a>
                </li>
            </ul>
            <div>
                <p>All emojis designed by <a href="https://openmoji.org/">OpenMoji</a> – the open-source emoji and icon
                    project. License: <a
                            href="https://creativecommons.org/licenses/by-sa/4.0/#">CC BY-SA 4.0</a></p>
            </div>
        </div>
    </div>
{% endblock %}